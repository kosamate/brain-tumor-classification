Mély tanulás alkalmazása
A dolgozatom fontos aspektusa a mesterséges intelligencia használata. Mesterséges intelligenciának nevezünk minden olyan mesterségesen létrehozott logikát, ami emberi beavatkozás nélkül, önállóan képes döntéseket hozni. A fogalom nem csak számítógépes intelligenciákra terjed ki, de manapság ezen a területen van a legnagyobb szintű felhasználása. Mesterséges intelligencia készíthető egy működő logika implementálásával és tanítással. A gépi tanulás a mesterséges intelligencia alkalmazásának azon változata, amikor előzetes tapasztaltból képes tanulni a számítógépes rendszer.
Én a dolgozatomban a mély tanulással, azon belül is konvolúciós neurális hálózatokkal dolgozom. A mély tanulás a gépi tanulás egy részhalmaza. A munkám során kétfajta feladatot valósítok meg: osztályozást és szegmentálást.
Osztályozásnak (classification) nevezzük azt az esetet, amikor előre definiált osztályokba szeretnénk besorolni a képen látható objektumot. Ezekben az esetekben képen egy példány szerepel. A hálózat bemenete egy kép, amely tetszőlegesen lehet színes vagy szürke árnyalatos. A kimenetek száma pedig megegyezik a definiált osztályok számával. Mindegyik kimenet egy osztályt reprezentál. A legnagyobb aktivitású kimenet határozza meg, hogy melyik osztályba sorolja a mesterséges intelligencia a képet.
A szegmentlás vagy pontosabb nevén szemantikai szegmentálás (semantic segmentation) beazonosítja, hogy a képen látható egyes pixelek melyik osztályba tartoznak. A mesterséges intelligencia kimenete egy olyan mátrix, amely mérete megegyezik a bemeneti mátrix méretével és minden eleme az adott pixel osztályáról hordoz információt .
 
1. ábra Szemantikai és példány szegmentálás közötti különbség. [2]
A szemantikai szegmentálásnak nem feladata megkülönböztetni az egyes példányokat, csupán a képpontokra érvényes osztályt. A példány szegmentálás (instance segmentation) esetén azonban a képen látható egyes példányok is megkülönböztetésre kerülnek függetlenül attól, hogy ugyanannak az osztálynak képzik a részét. 
Mély tanulás működése
A tanításhoz egy hálózatot szükséges készíteni. A neurális hálózatok alapötlete az emberi agyban található neuronok modellezése. Egy hálózatnak van bemenete (input) és kimenete (output). A bemenet és a kimenet között mély tanulás esetén rejtett rétegek találhatóak. A rétegek többsége rendelkezik egy vagy több tanítható paraméterrel (weights), amelyet a tanítás során hangolhatunk annak reményében, hogy a paraméter változtatásával pontosabb működést érünk el.
A mély tanítás tovább bontható felügyelt és nem felügyelt tanításra. A felügyelt tanítás esetén előzetes információval rendelkezünk, hogy egy bemenetre mi lenne az elvárt kimenet, ezeket az adatokat nevezzük tanító adatoknak. A tanítás során ezeket az adatokat adatcsomagokba (batch) rendezzük, így egy lépésben több különböző adatot is átfuttatunk a hálózaton. Minden lépésben megvizsgáljuk, hogy a hálózat által becsült kimenet mennyire esik közel az elvárt kimenethez.
A hiba mértékének jellemzésére egy hibafüggvény (loss function) definiálunk. A hibafüggvény bemenete az elvárt és a hálózat által szolgáltatott kimenet. A függvény kimenete egy valós szám. Definíció szerint a kisebb hibaértékű eredmény, kisebb hibát jelez előre.
A hiba ismeretében javíthatunk a hálózat egyes paraméterein. Ennek a lépésnek a neve hiba-visszaterjesztés (backpropagation), ami a legtöbb számítási kapacitást igénylő művelet. A teljesség igénye nélkül, a hiba-visszaterjesztés során a hálózat kimeneti rétege felül indulva deriválással rétegről-rétegre meghatározható, hogy az egyes rétegek tanítható paraméterei mekkora mértékben járultak hozzá a keletkező hibához. Ezen hiba járulék függvényében javítást végzünk minden paraméteren.
A tanítható paraméterek frissítését az optimalizáló (optimizer) végzi. Az optimalizáló meghatározza, hogy hogyan és milyen ütemben változtatjuk meg a tanítható paramétereket. Általában az optimalizáló fontos bemeneti paramétere a tanulási sebesség (learing rate). A hiba-visszaterjesztés és a paraméterek változtatása minden egyes batch lefuttatása után megtörténik.
Az optimalizáló és a veszteségfüggvény kiválasztása mindig az adott problémától és elkészített hálózattól függ.
A tanítás esetén gyakran megfigyelhető probléma a túltanulás (overfitting) jelensége, amely során a hálózat túlságosan is a tanító adatokat tanulja meg, így szokás ezt magolásnak is nevezni. Ebben az esetben a hálózat olyan jellemzőket tanul meg, ami irreleváns az adott probléma szempontjából, azonban a tanító adatok alapján a hálózat ezzel ellentétes információkat kap. A túltanulás előállhat, ha túl sokszor futtatjuk végig a hálózaton a tanító adatokat.  A túltanulás elkerülésére bemutatok speciális védekezési lehetőségeket a további fejezetekben.
A tanító adatcsomagokat több lépésben is lefuttatjuk a hálózaton, hogy lépésről-lépésre javítsuk annak pontosságát. Egy korszaknak (epoch) nevezzük azt az iterációs lépést, amikor a teljes tanító adathalmazt lefuttattuk a modellen.
Konvolúciós neurális hálózatok felépítése
A konvolúciós neurális hálózatok olyan mély tanulás alapú neurális hálózatok, amelyekben konvolúciót hajtunk végre a hálózat elején a bemeneti adatokon. A konvolúción kívül más speciális rétegek is megjelennek. [3] 
 
2. ábra Példa egy konvolúciós neurális hálózat felépítésére. [3]
Konvolúciós neurális hálózatokat gyakran használják képfeldolgozásra. Én az alábbiakban a 2 dimenziós esetekre mutatom be a konvolúciós neurális hálózatokat, de ezek általánosíthatóak több dimenzióra is. A konvolúciós rétegekkel elérhető a bemeneti vektor (feature vector) méretének csökkentése anélkül, hogy jelentős információt vesztenénk. A kisebb méretű adatokkal az erőforrásigényes számítási rétegek sokkal jobb teljesítmény mellett használhatóak.
Konvolúciós réteg
Egy dimenzióban a diszkrét konvolúció az alábbi képlettel adható meg:
█((f*g)[n]= ∑_k▒f[k]g[n-k]   #(2.1) )
A konvolúció kiterjeszthető 2 és több dimenziókra is. 2 dimenzióban egy szűrőt (kernelt) helyezünk el a bemeneti mátrix minden olyan pontjába, ahol a kernel és bemeneti mátrix értékei fedik egymást. Az egy dimenziós esethez hasonlóan képezzük a bemeneti mátrix és a kernel azonos pozíciójában lévő elemek szorzatait, majd ezeket a szorzatokat összegezzük. Az összegzés után előáll a kimeneti mátrix (feature map) egy eleme. A kernel mozgatásával a bemeneti mátrix különböző pozícióiban képezzük a szorzat összegeket, így előáll a kimeneti mátrix az ábrának megfelelően.
 
2.3. ábra 2 dimenziós konvolúció szemléltetése. [4]
 A szűrő és a bemenet méretétől függ a kapott kimenet mérete. Ha a bementünk egy n x n nagyságú kép és egy k x k méretű szűrőt alkalmazunk rajta, akkor a kimenetként egy (n – k + 1) x (n – k + 1) méretű mátrixot kapunk.
A konvolúció elvégzése során számos paraméter változtatásával hatással lehetünk az elvégzett műveletre. Jól megválasztott paraméterek azt eredményezik, hogy a bemeneti képből kevés információt veszítünk, de a feldolgozandó adatok mérete jelentősen csökken.
Egy konvolúciós réteg fontos paramétere a padding. A konvolúció végrehajtásakor a bemeneti mátrix szélén elhelyezkedő értékek kevesebb kimeneti elem előállításban vesznek részt, mint a mátrix belsejében elhelyezkedő elemek. Padding beállításával a bemeneti mátrix széleit kiegészítjük padding sorokkal, illetve oszlopokkal. A megnövelt mátrix mérettel a konvolúció elvégzésekor az eredeti mátrix szélein elhelyezkedő értékek hatása is hasonló számmal jelenik meg a kimeneti értékekben, mint a belső értékek esetén. A padding értéke megmutatja, hogy hány oszloppal, illetve sorral növeltük meg a bemeneti mátrixot. A padding értékeket választhatjuk 0-nak, vagy pedig képezhetjük őket az eredeti mátrix széleinek megduplázásával. A padding hasznos, azonban növeli a kimenet méretét. A kimeneti mátrix a padding számának kétszeresével növekszik.
A korábban említettek alapján a fő cél a kimenet méretének csökkentése. Stride érték is beállítható a kernelre. A stride hatására csökken a kimeneti mátrix mérete. A stride azt határozza meg, hogy a kernelt mekkora lépésekben mozgatjuk a bemeneten. Stride beállításával nagy mértékben csökkenteni tudjuk a kimenet méretét, ugyanis a stride értékével közel fordított arányban változik a kimenet mérete.
A dilation paraméterrel beállítható, hogy a szűrő elemek milyen távol helyezkedjenek el egymástól. Ennek a paraméternek a használatakor nem a szomszédos bemeneti értékek kerülnek bele a számítási folyamatba. A dilation növelése is csökkenti kimeneti mátrix méretét.
Az említett paraméterek hatását összegezve az alábbi képlettel számolható ki a kimeneti mátrix mérete:
█(output= (output+2*padding-dilation*(filter-1)-1)/stride+ 1 #(2.2) )
Több dimenziós esetre is kiterjeszthető a konvolúció. Az analógiát alkalmazva egy n dimenziós tenzor esetén n dimenziós szűrőre van szükség és a kimenet is n dimenziós tenzor lesz.
Pooling réteg
A pooling réteg alkalmazásának a szerepe hasonló a konvolúciós rétegekéhez: csökkenteni a kimenet méretét, így csökkentve a szükséges számítási kapacitást a további rétegek esetében. Ennek a rétegnek a használatakor is egy szűrőt (kernelt) használunk, azonban ebben az esetben a szűrőnek csak a mérete lényeges, szűrő értékek nem vesznek részt a pooling rétegek számításaiban. A szűrő által fedett elemekből képződik közvetlenül a kimenet. A kernelt azonban mindig szűrő méretével megegyező mértékben mozgatjuk a bementi mátrixon, így a bementi mátrix minden eleme csak egy kimeneti mátrix érték számításában vesz részt.
Kétféle poolingot használunk a gyakorlatban. Max pooling esetén a szűrő által fedett legnagyobb érték kerül kiválasztásra. Average pooling esetén pedig a szűrő által fedett elemek átlaga lesz a kimeneti érték.
A pooling rétegek használatával a kimeneti mátrix mérete nagy mértékben csökkenthető, hiszen egy k x k méretű szűrő esetén k2-tel csökken a kimeneti mátrix.
 
2.4. ábra Pooling [3]
Aktivációs függvények

Dropout réteg 
A dropout réteg feladat, hogy megakadályozza a hálózat túltanulását. [5] Használatakor minden iterációban véletlenszerűen nullázza az egyes neuronok értékét. Paraméterként megadható, hogy a réteg milyen mértékben (rate) avatkozzon be. Annak érdekében, hogy a réteg neuronok aktivációjának összege ne változzon a nullává nem alakított neuronok aktivációját 1/((1-rate)) tényezővel megnöveli a réteg.
Flatten réteg
Az eddigi rétegek esetén az adatot mátrixként kezeltük. A konvolúciós neurális hálózat végén azonban egy vektort várunk majd eredményként a problémától függően. A szétterítő (flatten) réteg a mátrixból egy oszlop vektort készít. A réteg alkalmazásával az elem szám nem csökken.
Teljesen összekapcsolt réteg
Ez a réteg a hálózat vége felé egy flatten réteg után helyezkedik el. A teljesen összekapcsolt réteg (fully connected layer) olyan réteg, amelynek az összes neuronja kapcsolatban áll a következő réteg minden egyes neuronjával. Ha ezt már korábbi réteg esetén is megtettük volna, akkor rengeteg neuronunk lett volna rengeteg összeköttetéssel. A hálózat utolsó rétege is egy teljesen összekapcsolt réteg.
Softmax és LogSoftmax réteg [6]
A Softmax egy olyan matematikai függvény, amely egy valós számokból álló vektort alakít egy valószínűségi eloszlás vektorrá, azaz egy olyan vektorrá, aminek az elemeink az összege 1. A valószínűségi eloszlás vektorban minden elem [0, 1] intervallumon belüli érték.
█(σ_softmax (z_i )= e^(z_i )/(∑_(j=1)^K▒e^(z_j )  )  #(2.3) )
ahol K a vektor elemeinek száma, zi pedig az egyes elemek értéke.
A függvény nagyon jól használható osztályozási esetekben a neurális hálózat végén. A végeredményben előálló N érték az N osztály valószínűségét jellemzi. A softmax függvénnyel lehetőség nyílik a kimenet normalizálására és valódi valószínűségi eloszlás visszaadására.
A logsoftmax függvény funkcionális szerepe hasonló a mesterséges intelligencia területén, azonban megoldást nyújt a softmax függvény alkalmazás esetén gyakran előforduló problémákra. Ilyen probléma lehet, hogy túl nagy valós számok szerepelnek a vektorban. A nagy valós számú kitevőre emelés pedig költséges matematikai művelet, és numerikusan nem stabil. Adott esetben az ábrázolási tartomány szélét is elérhetjük, így a számítás nan értékkel folytatódik tovább. A logsoftmax függvény lényegében a logaritmusa a softmax függvénynek.
█(σ_logsoftmax (z_i )=log⁡(e^(z_i )/(∑_(j=1)^K▒e^(z_j ) ))= z_i-log⁡(∑_(j=1)^K▒e^(z_j ) )  #(2.4) )
ha a zárójelben szereplő kifejezést becsüljük a legnagyobb értékű elemmel, akkor az alábbi egyszerű összefüggésre jutunk:
█(z_i-max⁡(z)  #(2.5) )
Logsoftmax függvény esetén az előálló végeredmény nem valószínűségi eloszlás vektor, hanem egy logaritmus valószínűség vektor. A jelentése a logaritmikus valószínűségnek, hogy logaritmikus skálán ábrázoljuk a valószínűségeket. Ezen ábrázolás gyakorlati szempontból praktikusabb.
Batch normalizáló réteg [7]
A batch normalizáló réteg gyorsabbá és stabilabbá teszi a hálózat tanulási folyamatát. Ezt azáltal éri el, hogy normalizálja az egyes mini-batch-ek belső aktivációit a hálózat rétegei között. Használatával elkerülhető a túltanulás jelensége. A réteg 2 tanítható paraméterrel rendelkezik.
Szegmentálás
A gépi tanulásról szóló fejezetben bemutattam a szegmentálás cél kitűzését. Ebben a fejezetben kifejtem, hogy milyen technológiai megvalósítások érhetőek el ezen a területen, amelyekre a munkám során is nagy mértékben támaszkodtam. A szegmentálás során a színcsatornák kivételével a bemenet és a kimenet dimenziója megegyezik egymással.
U-Net [8]
Az U-Net gyakorlatilag egy konvolúciós neurális hálózat. amelyet főként a szegmentáció területén használnak. Az elnevezés abból származik, hogy a hálózat rétegei szimmetrikusan helyezkednek el, emiatt sok esetben egy U betű alakját követő elrendezésben ábrázolják a különböző U-Net hálózatokat. Ezen hálózatok ismertető jele, hogy a hálózat második részében felül mintavételezést (upsampling) alkalmaznak a kimenet megfelelő dimenziójának elérése érdekében. A felül mintavételezésre több réteg is használható.
